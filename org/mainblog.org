#+title: Main Blog
#+author: Rylan Lens Kellogg
#+description: A blog for all things Lens_r.
#+created: <2023-05-29 Mon>

* HOW TO WRITE POSTS

First of all, a kind of overview of how this works. Each top-level heading (one star) with the "blogpost" tag will be exported by the publishing system to its own file, and have its own webpage on the website that will be linked to from the main blog page. Everything else /does/ get exported to HTML, but only within the (not linked) =mainblog.html=. Which means readers of the blog won't see it :^).

Here's an example blog post. It's quite simple. It has a title (the headline of the top-level org heading). It has a =blogpost= tag. It has a unique =CUSTOM_ID= property and the date published (inserted with =M-x org-time-stamp-inactive=, or at least in the format of it).

Anything within this subtree will then be a part of the blog post. For now, there aren't any fancy CSS things going on, except for code being syntax highlighted. In the future I may add more properties so that certain pieces of text may be marked up in unique ways, but for now, we're going bare org-mode.

#+begin_src org
  \* Ooops, out of order! :blogpost:
    :PROPERTIES:
    :CUSTOM_ID: outofordertest
    :PUBLISHED: [2023-05-29 Mon 12:40]
    :MODIFIED: [2023-05-29 Mon 12:40]
    :OG_IMAGE: images/myimage.png
    :OG_DESCRIPTION: A short hook to make readers click the link!
    :POST_TAGS: lorem, ipsum, dolor, sit, amet
    :END:

    Hmm, will this be in order?
#+end_src

NOTE: The OG_IMAGE property is optional, but, if provided, sets the open graph/twitter card image (og:image) that will be displayed when the page is linked to from social media sites. OG_DESCRIPTION may also be set in order to explicitly set og:description.

NOTE: POST_TAGS property is not yet used yet, but will eventually be used for users to be able to find posts by tag.


* Fun Titles / Idea Dump

** How I Built Rome in a Day
[2023-06-01 Thu 16:14]

** How I Manage My Finances From the Command Line
[2023-07-07 Fri 08:42]

** I Use Org Mode For Everything From Work to Play
[2023-07-07 Fri 08:42]

** Conlang Showcase: Tlai-Tli (early)
[2023-07-07 Fri 08:43]

** Calculating My Content Creation Wage (it's negative)
[2023-07-07 Fri 08:43]

* Making Mistakes :WIPpost:
:PROPERTIES:
:CUSTOM_ID: makingmistkaes
:PUBLISHED: [2023-07-07 Fri 08:39]
:MODIFIED: [2023-07-07 Fri 08:41]
:OG_DESCRIPTION: A little too much about my life, I guess
:POST_TAGS: life
:END:



* Getting Comfortable With the Uncomfortable :blogpost:
:PROPERTIES:
:CUSTOM_ID: comfortablewhenuncomfortable
:PUBLISHED: [2023-06-22 Thu 15:29]
:MODIFIED: [2023-06-22 Thu 15:31]
:OG_DESCRIPTION: Getting Comfortable With the Uncomfortable, or How I Stay Sane Despite This Insane World
:POST_TAGS: life
:END:

I got a new job.

/Yay.../

I am now a Senior Engineer, writing software the likes of which I have never tried to write before. Everyday I am (a little) stressed with the ever-increasing list of things to get do, most of which I have never ~A.~ tried before or even ~B.~ heard of in my life.

I wrote an entire OS, but that didn't teach me how to write a promise-based API in JavaScript.

I wrote an entire compiler, but that didn't teach me how to use industry-level tools to create a scalable app that ebbs and flows as the active users fluctuate.

What doing those projects /did/ teach me was far more important than those minute details that I've picked up in my first week or two on the job.

** Solo Dolo

When you're all alone, your entire mindset changes. It is either "do" or it doesn't ever get done. When writing the kernel for LensorOS, I didn't have anyone to talk to in real life about it (at least anyone that would understand what the hell a filesystem even is, haha). This meant that most of my work was done alone; chicken-scratched writings in countless notebooks, solving problems that have been solved a million times before. Obviously, I didn't just gain knowledge spontaneously; I still utilised the resources I had available (like the OSDev wiki, among other things). When I say /alone/, I'm referring to the fact that, within those first four months, the only time I asked anybody for help on how to solve an issue was through a Reddit post that got no responses. With no contributors, no one to bounce ideas off of ... just /me/, within the eternal sunshine of my spotless mind, alone and unknowing of what I might find, I meandered onto a lesson that has stuck with me through time.

*/No one is there to save you/*.

For weeks I would battle a problem, flinging lines of code into file upon file, waiting for one---just /one/---solution to work. They often didn't. If I decided at this time, "I don't know how to do it, that means I /can't/ do it without help", then I wouldn't have /any/ programming projects. For every single one of these problems---these problems that caused me to yell in frustration, angrily write cuss words, and frantically etch my thoughts into disorganised page after disorganised page of my helpless notebook---I would go through a cycle. First, the idea would hit me; the excitement that came with it usually meant I got started right away on designing /some/ sort of system to implement this idea. After an initial design that I thought /might/ work, I'd jump right into implementing it. Usually, about half-way through implementing it, I would run into a road-block that made me realise my design could /never/ work, it's inherently flawed, and I /have/ to redesign it if I ever want this idea to come to fruition. This is where I often spent days\slash{}weeks, just desperately hoping for each attempt to be "the one".

In these weeks is when the frustration came: every time something didn't work as expected, the program crashed for seemingly no reason, or one piece of code being changed affected an entirely unrelated piece of code. In these moments, it wasn't only frustration that came, but also severe feelings of /failure/. My notebooks would be filled with phrases like "fucking loser", "idiot", "useless, pathetic fool", and every permutation of those you may possibly think of (and more). In those times, I heavily based my self-esteem on my accomplishments; if I didn't get what I wanted done that day, I'd be in an awful mood and wonder why I should even be alive. Needless to say, there were constant hardships throughout my journey, both mental and technical, but I never once gave up (or let my unhealthy thoughts make me do something I'd regret).

TL;DR: Oh, you have a question about how something works? Better just go ask your superior... WRONG! Chances are your superior doesn't know this specific, minute detail either, and will have to go through the same channels that /you/ could have to get the information, and then relay it back to you. Companies don't appreciate you off-loading your work onto your coworkers, /at all/; they want forward thinkers, people who get the job done before it has even started. A problem is only a problem if you haven't solved it yet!

** RTFM

#+begin_quote
Read The Fucking Manual
#+end_quote

I didn't have anyone to run to when I wanted to write an assembler for the Intercept compiler; I was the only one who implemented it. What I /did/ have that I could run to when I got lost was the ~Intel Software Developer's Manual~. That manual has everything you could possibly need to know about each and every possible instruction and encoding in x86\under{}64. *That manual was all I ever needed* to write an assembler for x86\under{}64.

Now, did I still take to Googling "how does modrm byte work x86\under{}64"? Of course! *There's no need to limit yourself to only one set of knowledge.* HOWEVER! The Intel SDM /does/ actually have everything that one may ever need to know about the ModRM byte. That is, this "rule" isn't meant to make you limit your sources of knowledge; it is to exercise the most likely avenue of getting that knowledge first, before you go crying to your peers/superiors about it.

** A Pile Of Garbage Smells Bad, but Still Smells

/Something/ is always better than nothing in the world of software engineering. GET STARTED. It doesn't matter if your whole idea of what is needed is wrong, your design is flawed, or anything like that; if you are dedicated, you will overcome all of these hurdles and get exactly where you want to go in the end (or somewhere near). For example, I had no idea how to serialise types for the Intercept compiler module description: that didn't stop me from starting up a stream and getting started on an implementation. About half-way through implementing my initial idea (Tag/Length/Value records), I realised that it wouldn't work *at all* for recursive types (i.e. a struct with a pointer to itself as a member). This didn't stop me; I continued implementing the flawed design, and then went back and thought about how I could change it to fix this new issue I've run into.

By continuing to work through the initial roadblock, I was able to reach a point where I made /progress/ towards my goal of serialising types, even though I hadn't yet succeeded. With this progress, Siraide was able to come up with a fix in just a few minutes, and I implemented that in less than an hour. From my broken, shoddy design emerged a functional one.

I am not a genius or something because my terrible designs end up working in the end; it's a simple fact that if you begin building something, you *will* run into issues along the way. *If you let these issues stop you, then you will never get anything done.* However, if you are able to begin working on a solution to /any/ problem, no matter how small, you will begin making progress on that problem. Keep doing that, and the problem will eventually be so small you don't even notice it.

* Assembly Is *Not* What It Seems :blogpost:
:PROPERTIES:
:CUSTOM_ID: assemblynotwhatseems
:PUBLISHED: [2023-06-03 Sat 12:39]
:MODIFIED: [2023-06-04 Sun 09:17]
:OG_DESCRIPTION: Exploring Assembly from a CPU's Point of View
:END:

Assembly is as close as you can get to the hardware, right? It basically tells you the exact instructions the CPU will run ... /right/?. Well, that may be what it seems like (and what people say), but not necessarily! Modern CPUs do so much more than just read an instruction and execute it, and that is what we will be exploring in this post.

** Why Is This the Prevailing Assumption?

Well, the reason assembly is thought of as "telling the hardware exactly what to do" is because ... well, it does, strictly from a programmer's point of view. But there's a distinction here that's important. It does not tell the CPU /how/ to do it. So while, yes, you may feed it an assembly instruction to add two numbers, any one CPU may choose to do an OR operation instead, if one of the numbers is known to be zero. This sort of behaviour, where the CPU can technically do whatever it wants as long as the expected result is acquired, allows for **lots** of optimisations that would not otherwise be possible. For related reading, see /sequential consistency/ and /unspecified behaviour/.

# While I would like to say the laborious work of the hardware designers to develop such complex optimisations and efficient systems is the /sole/ reason you can sit here with a browser open to multiple tabs, background services running, multiprocessing, etc, that's just not the case. Modern computers have sped up significantly, just at a base level. The clock speed of Intel's first CPU in 1971 was 740kHz, while the max clock speed of Intel's i7-13700k is 5.4GHz; that's a 7297x increase in the 52 years from 1971 to 2023. That's not to say the optimisations don't contribute to the amazing things computers are used for nowadays, just that humans have improved in a multitude of fields of study over this time, and all of these improvements have contributed to making computers that much faster, power-efficient, and more.

** How Can a CPU Do More Than One Thing at Once?

Imagining a naive CPU pipeline that does one thing at a time, we may arrive at something like this:

#+begin_example
.->  Decoder -> Processor  -.
`---------------------------+
#+end_example

The "decoder" step would decode the next instruction from memory, located by the instruction pointer, and the processor would get a decoded instruction with which to execute. While this /does/ work, it's also incredibly slower than it needs to be; for example, the decoder would decode an instruction /and then sit around doing nothing/ while the processor step computes the proper value. While the decoder is decoding the next instruction, the processor would sit around and do nothing.

NOTE: This is a whole topic in-and-of-itself. See [[https://en.wikipedia.org/wiki/Instruction_pipelining][Instruction Pipelining @wikipedia.org]]

*** Eliminating Dependencies

Sitting around doing nothing is *never* good. Luckily, we can attempt to make the situation better due to one simple fact: computations within the processor can't use or modify the instruction pointer directly (most of the time). This means the decoder /could/ start decoding the next instruction *before* the processor finishes processing the last instruction decoded. By the time the processor finishes processing, the decoder may already be ready with another instruction to execute. This increases the amount of time the processor spends doing valuable computations, making the CPU faster and computers go brrr.

#+begin_example
Decoder -> Decoded Instruction Queue
Pop(Decoded Instruction Queue) -> Processor
#+end_example

As you can see, this /eliminates a dependency/: the processor no longer relies on the decoder directly, and instead relies on the decoded instruction queue being populated. This idea, this /concept/, of eliminating a dependency reaches so far down into the roots of modern CPUs that I could not explain it in one article, or even five. Modern CPUs eliminate dependencies on a register's value by renaming registers temporarily (yes, even the "hardware registers" don't actually exist in hardware ... there is something called a register file and it contains /actual/ hardware registers and the "name" of the register it's currently bound to; this technique is called /register banking/). This register renaming fixes our little caveat above with the instruction pointer being used in a computation. Just copy the value of the instruction pointer into a new register and rename that register to the instruction pointer for that instruction. Poof! No need to operate directly on the instruction pointer. In fact, this works for all registers.

Now, you might be wondering, what is the advantage of eliminating a dependency on a register's value? This is where the next big step in computational speed comes from.

*** Out-Of-Order Execution

That's right; by eliminating an instruction's dependency on a register, we can actually *execute* that instruction at the same time as another instruction, given they don't have dependencies on one another. Let's take a look at this in actual x86\under{}64 assembly (in Intel syntax today, for funsies).

#+begin_src asm
0      mov rax, [my_ptr]           ;;#; rax := memory[my_ptr]
1      add rax, 2                  ;;#; rax := rax + 2
2      mov [my_ptr + 8], rax       ;;#; memory[my_ptr + 8] := rax
3      mov rax, [my_other_ptr]     ;;#; rax := memory[my_other_ptr]
4      add rax, 4                  ;;#; rax := rax + 4
5      mov [my_other_ptr + 8], rax ;;#; memory[my_other_ptr] := rax
#+end_src

Attempting to eliminate dependencies in the above code without renaming registers doesn't gain us much; ~rax~ is used in *every* instruction, and therefore each instruction is dependant on the value of ~rax~ in the last instruction. Some instructions don't alter the register operand (like storing to memory), but they still require the value of ~rax~ to be what it was at the last assignment; because ~rax~ can't be reassigned, this store would still not able to be done in parallel with an instruction that sets the value of ~rax~.

/This/ is where register renaming takes the spotlight. Because the x86\under{}64 CPU is smart enough to know which instructions set a register and which ones just use them, it can analyse the code it's about to execute and determine register dependencies. For example, instruction 0 sets the value of ~rax~ and has no dependencies. Instruction 1 sets the value of ~rax~ as well, but this time has a register dependency on the value of ~rax~ set by instruction 0. So instruction 1 /depends/ on instruction 0 already having been executed, and they cannot be executed out-of-order (or in parallel). It's a similar situation for instruction 2, as it depends on the value of ~rax~ set in instruction 1. However, instruction 3 is where it gets *interesting*. With the value of ~rax~ being set again, but this time from another place in memory, this means that any dependency on the old ~rax~ is broken. So instruction 3 has no dependencies, just like instruction 0. Instruction 4 is nearly identical to instruction 1, except this time it's dependent on the value of ~rax~ set in instruction 3. Same story for instruction 5, except dependent on instruction 4. Okay, so we can determine the register dependencies of an instruction ... but what has all this analysis got us? To showcase the value gained from doing this analysis, let's go through and give a unique name to each /value/ of ~rax~ that was depended upon.

#+begin_src asm
0      mov r1, [my_ptr]            ;;#; r1 := memory[my_ptr]
1      add r1, 2                   ;;#; r1 := r1 + 2
2      mov [my_ptr + 8], r1        ;;#; memory[my_ptr + 8] := r1
3      mov r2, [my_other_ptr]      ;;#; r2 := memory[my_other_ptr]
4      add r2, 4                   ;;#; r2 := r2 + 4
5      mov [my_other_ptr + 8], r2  ;;#; memory[my_other_ptr] := r2
#+end_src

Now, with this done, the CPU is smart enough to notice something: instructions 0 through 2 and 3 through 5 are two blocks of instructions that start with /no/ register dependencies.

#+begin_example
0 sets r1
1 uses r1 and sets r1
2 uses r1

3 sets r2
4 uses r2 and sets r2
5 uses r2
#+end_example

As neither of these blocks of instructions depend on each other for any values of any register (CPU state), this means they *can* be executed out-of-order. So, if the L1 cache has the memory at ~my_other_ptr~ already loaded, for example, the CPU could choose to execute the block of instructions that uses that memory more first, taking advantage of the already-populated cache. And that's just for a single CPU with a single logical/arithmetic unit.

At some point, humans were smart enough to realise that a CPU already has a clock, registers, instructions, etc, but, /for some reason/, only ever computes one instruction which operates on one or two registers per clock cycle. By inserting more actual logical and arithmetic units within a single CPU, it's possible for a single computational unit to compute /more than one/ calculation at a time, operating on more than just one or two of its registers. That is, for two sequential ~add~ instructions that have no dependencies on each other, it's *vastly* more efficient to send each to its own ALU and have a single clock cycle cause both of them to do their respective computations, getting both results at the same time. This idea even extends past ~add~ instructions. For example, the instruction decoder could be duplicated, allowing for multiple instructions to be decoded at once.

With modern processors, this is taken even one step further: the "CPU" has /multiple/ CPUs inside of it, each with their own set of ALUs, register files, and more. Generally, the OS chooses the CPU it starts on as the "main" CPU, and that CPU is used to dispatch heavy computations between the rest. It is up to the OS kernel how this is actually accomplished, and what the other CPUs are used for: this is the job of the /scheduler/ (another topic that I could write a million articles on and barely scratch the surface).

Modern CPUs are to assembly what C is to Python. You can use C to implement Python, but it will be a lot more verbose, detailed, and complicated than any equivalent you could come up with in Python. Modern CPUs look at assembly and /wish/ they could operate at such an abstract level, while assembly sees the CPU simply as a means to an end. So, the next time you write (or read) some assembly, remember that the CPU has other things in mind than just src_asm[:exports code]{ add rax, rax}.

Anyway, thank you for reading this post on assembly. If you enjoyed it, I make Twitch and YouTube content that you might also enjoy. To stay tuned when more posts come out, there is an RSS feed you can subscribe to.


* What /Is/ a Program? :blogpost:
:PROPERTIES:
:CUSTOM_ID: whatsaprogram
:PUBLISHED: [2023-05-29 Mon 08:41]
:MODIFIED: [2023-06-04 Sun 09:17]
:END:

This may seem obvious, but it turns out to be quite ... complex.

#+begin_src c
  int main() {
    return 69;
  }
#+end_src

Is the above code a "program"? Most will say yes, in my experience. This immediately throws a wrench into most /obvious/ definitions of program.

The code above is not executable; it's simply plain-text within a file. Well, then maybe a program /isn't/ necessarily executable, but /some/ programs /may/ be executed. So "something executable on a computer" isn't really a valid definition of "program".

Some, from here, may expand the definition to "something that may be eventually executable on a computer (after some set of transformations)". Another issue arises, however, if we look at the following example.

#+begin_src c
  int main() {
    return 69
  }
#+end_src

Is the above code a "program"? If we follow the "eventually executable" definition, it /isn't/. There is a syntax error, as the ~return~ statement is not terminated with a semi-colon. This code, therefore, isn't compileable; it's an "ill-formed program" according to the C standard. So, as we can see, some programs (without changing the source) are not *ever* executable.

So, a program isn't necessarily well-formed, a la compileable, and a program isn't necessarily executable. We're right back to the start: what /is/ a program? To me, someone who "writes programs", it would seem that the things I write would be programs. So let's take this top-down approach, and find out what we already call programs, and /only then/ begin to tighten the definition without excluding anything. What things might be a program?

- An executable file (in any format) is definitely a program.
- An object file may contain portions of or all of a program or programs.
- Source code is thought of as a program ("programmers write programs").


From there, then, let's try to fit a definition to this set of things. There's one thing you may notice: they /all/ have code in them ... just in *very* different forms. An executable file has machine code in it (among other things that tell the computer /how/ to execute the file). The object file has machine code in it (or intermediate representation if using link-time optimisation). And finally, for the source code, it's even in the name. So, as /vague/ as it is, I think that we can begin to narrow our idea of "program".

A "program" is /some form/ of instructions meant for a computer to do computations.

So that C code up above? Well, it's only written with the intent that that sequence of tokens in that language will produce a given computation. "Code" is just instructions meant for a computer, no matter if that is machine code, C code, or LISP.

However, this definition /does/ come with its own host of caveats. For example, the source code of a program fits the definition of "instructions meant for a computer", but so does the executable file generated after compiling that code. In that case, are there /two/ programs? Or just one program in two different formats? I think this is a question of philosophy, truthfully. To me, it makes the most sense that there /are/ two programs, they just have a set of instructions in different formats that /happen/ to tell the computer to do the same thing (unless your compiler is borked/I wrote it).

** Etymology of "program"

The word "program" is derived from Greek /programma/, meaning "a public written notice". (See? Even the Ancient Greeks knew that software should be open to the public :Ãž.) In the 1600s, it was used in concert and theatre, referring to an outline of what was going to happen that day (i.e. features presented, persons participating, etc). We can see from its early use that a program defines what is going to happen during a performance.

In the mid-1900s, when computers came about (thanks Alan), it stood to reason that something that tells you what the computer is going to do while it is running (during its /performance/) would be a /computer program/. And this is when it kind of got out of hand. Computers back then used punch cards as input; those punch cards, naturally, became known as programs. And at this point, everything still makes relative sense. It's not confusing what a computer program is.

*And that's exactly when it got confusing.*

Computers seriously blossomed in the years following its discovery/invention. New hardware, new software, good times. Computers upgraded from full-room behemoths that munch on punch-cards to somewhat-reasonable (although still large) machines programmed in assembly. And with this shift came an important distinction: programmers now write assembly code, but the computer no longer executes that directly. The assembly is first /assembled/ into machine code, and only then is that executed by the computer. The people who used to punch cards to tell the computer what to do? Well now they wrote source code. But to them, they were still doing the same thing: telling computers to do some computations. "Something a programmer writes" /must/ be a "program", so therefore the source code a programmer writes /must/ be a "program". On the other end, a computer would read a punch card and do execution/computation based on it. That means that the compiler's output, the actual thing fed to the computer to make it do computation, /also/ ended up being called a "program", even though these two things have been separated in reality.

Because there was no longer a physical punch card tied to a "program", the original meaning of "program" (a printed list of features, persons participating, etc. at a concert/theatre) no longer applies /at all/. The /concept/ stayed (a list of things that tells humans what's going to happen), but the actual meaning was transformed greatly. At this point, arbitrary bits on some magnetic tape were now a program. The baby was, in fact, thrown out with the bath-water.

** A Definition of "program" that I Am Comfortable With

To me, there /isn't/ a clear-cut definition of program. No matter which one you choose, there are unintuitive corner-cases. However! That does not stop me from /choosing/ a definition that I am comfortable with.

What if "program" actually equates to "instructions that tell a computer to do computations". While this /is/ incredibly vague, it is also just specific enough. For example, when you write C code, you are attempting to instruct the computer on how to do execution/computation in order to give you the result you want. And when you compile that C code into an executable, the executable also contains instructions that tells a computer how to do computations, just in a different format.

As with every definition of program, there are imperfect corner cases, but this is one I'm okay with: the /source code/ and the /executable produced from that source code/ are entirely separate programs that happen to have instructions within them that produce the same result (assuming a well-written compiler).

